# 概述

各位评委老师，大家好，我是李永宁。

我的述职内容分为三部分，个人简单介绍、重要工作、个人总结和未来规划。

我是 22年07月份 入职的，到现在差不多三年了，职级是 S7，本次拟晋升 S8。

这期间的核心工作有以下四部分：

* 基于前后端协同设计的 PDF 生成系统，相比于常规方案业务提效至少超过 50%+，100% 还原设计稿，并且填补了社区的技术空白，赋能公司多条业务线
* 设计了 SaaS 私有化一体化架构方案，支撑企业安全云 SaaS + 私有化双重能力，满足多类型用户诉求，提升交付效率和业务增长弹性
* 微前端架构落地，首先这是一套完整的架构方案，并且在团队内构建了统一的技术栈和开发规范，显著提升技术架构的扩展性和可维护性以及业务迭代速度
* 构建系统稳定性治理体系，从技术和流程上双维度推动稳定性建设，实现全年 0 事故

期间也取得了一些荣誉：拿过一次互联网事业群优秀员工、一次公司级金牌团队、两次部门优秀员工。

# PDF 生成系统

首先是 PDF 生成系统，该系统是为了支撑公司安全运维托管服务战略而建设的，该业务需要通过一份视觉精美的 PDF 文件来给用户传递我们的业务价值。

技术选型上可以分为两类，一是原生方案，二是转换方案，原生方案的绝对优势是性能，但在复杂排版和样式上能力不足；转换方案是将内容通过中间媒介转换成 PDF 文件，比如 Word 转 PDF、HTML/CSS 转 PDF。最后从各个方案的优缺点、业务诉求、团队性质、人力和时间成本等各方面综合考虑，选择了基于浏览器打印系统的 PDF 生成方案。可以看到该方案将这类型的需求变成了一个普通的 Web 开发，需求过来之后，前端负责写页面，服务端提供数据接口，完成开发之后，将页面的 URL 交给系统，系统将会生成一份页面对应的 PDF 文件。

这是系统的整体架构图，分为三部分，一是接入方，二是 PDF 生成服务，三是配置服务，配置服务负责管理和维护接入方的所有信息，比如哪些页面要生成 PDF 文件；接入方带着 APPID 请求 PDF 生成服务，PDF 生成服务从配置服务取到接入方的所有信息，并将接入方指定的页面转换成 PDF 文件，最后将文件回传给接入方。

该方案的第一个复杂点就是目录页，尤其是带有跳转能力和准确页码的目录页，这在技术社区完全是个空白，可以发现社区里面讲 PDF 生成的博主有很多，但没有人讲目录页的方案。我这边是基于 HTML 锚点来实现的目录页超链接效果，而准确的页码是通过制定设计规范 + 页面缩放 和 高度补偿算法来实现的。

方案的第二个复杂点就是 PDF 内容页支持由多个前端页面组成。PDF 生成系统的核心逻辑是：第一步在浏览器中打开指定页面，第二步将当前页面转换成 PDF 文件，和浏览器中右键点打印的效果一样，所以页面和 PDF 文件是一一对应的。这背后就有个问题了，如果一个 PDF 文件是由几十个模块组成，那它就一定对应着一个及其庞大的前端页面，这就会导致接入方的前端项目在可维护性、性能上面出现严重问题，也限制了项目的架构和编码。但该限制是来自于技术方案上的限制，一是浏览器层面的限制，一个页面对应一份 PDF 文件，二是目录页方案的限制，目录页通过锚点实现的超链接效果。解决方案是在系统中增加一个胶水层，我的目的是期望将多个页面打印成一份 PDF 文件，那就通过胶水层将多个页面合并成一个页面，然后将这个合成后的页面打印成 PDF 文件。但这时候就出现个问题，由多个页面组成的合成页，出现了逻辑冲突和样式污染问题，这点在前端里其实很好理解，这里我是通过实现了一个沙箱来解决的，将各个页面包裹在独立的沙箱中，从而避免冲突问题。

该系统服务于公司多个业务线，比如安全云自己、办公、政企 MSS 等，共计生成了 10w+ 份 PDF 文件；也填补了社区的技术空白，为社区提供了一套完整的 PDF 生成解决方案，在社区也获得了不少认可。这是政企 MSS 业务接入前后的变化，可以看到视觉效果的明显差异，效率上也是，接入前是服务端同学自己弄，他们选用的是 Word 转 PDF 的方案，前端帮不上忙，接入后前后端各司其职，效率提升一倍不止。

# SaaS 业务私有化架构

第二件事情是 SaaS 业务私有化架构。首先该方案针对的目标用户是有预算，但不够完整私有化部署的预算，但是呢一定要有私有化部署的访问效果，但客户和我们在资金以及人力上都扛不住私有化部署的成本，另外一点就是 SaaS 服务的技术架构也无法支持私有化部署。其实用户要就是一个私有化部署的访问效果，比如在浏览器中输入一个内网 IP 能访问企业安全云的 SaaS 控制台。

这是企业安全云 SaaS 控制台的现状，一句话总结就是互联网用户通过公网域名直接访问对应的服务。这些服务包括前端自己的、服务端的 API、CDN 的资源、第三方的服务等，这些服务在项目中使用的都是绝对地址。

那这时候用户在不通外网的内网中访问时，所有的外网请求都出不去，现有方案不通。

所以，项目的挑战点就是：在非私有化部署的前提下，如何让内网用户通过内网 IP 访问企业安全云的 SaaS 服务，让 SaaS 服务看起来就像部署在内网环境一样。

答案就是内网代理，在内网搭一套代理服务，由代理服务负责链接内网和公网，内网用户访问的也是这个代理服务的 IP，也符合用户私有化部署访问效果的诉求。

那怎么实现呢？入口文件好说，直接转发对应的请求就行了，但写死在前端项目中众多绝对地址的网络请求呢？比如 CDN 资源、服务端 API、第三方服务等，解决方案有三个。

方案 1，直接改前端项目，首先将众多绝对地址改为相对地址，比如 API、CDN 资源等，另外私有化部署时前端的静态资源不上 CDN，并给这些请求增加特定的前缀，方便在代理服务上做拦截转发，最后就是对接的各个 SaaS 服务也配合采用同样的方式做一遍适配，这样系统发出的所有网络请求就都会经过代理服务，再由代理服务根据前缀转发到对应的公网服务上。这个方案是最容易想到的，看起来也就是个纯体力活，但其实是成本最高的一个方案，系统中十来个项目，超百万行代码，这个更改成本和回归范围完全不可控，而且一些第三方服务根本不具备更改的条件。

方案 2，更改客户的网络配置，将企业安全云的相关公网流量全部解析到代理服务上，剩下的工作就由我们在代理服务上来承接就好了，比如流量的转发、跨域的配置等。这套方案的目标是让每个客户的运维团队来配合我们，但客户不干，因为这套方案对客户的成本来说就很高了，尤其是后续的维护，另外可行性也很低，我们的客户基本上都是中小微企业，技术能力比较薄弱，而且拿税务局这个项目来说，全国大大小小有几百上千家税务局，这个实施成本太高，所以客户不愿意。

方案 3，拦截器结合代理服务，在浏览器中设置一套拦截器，将所有的网络请求由拦截器转发到代理服务上，再由代理服务转发到对应的公网服务上。

这里的核心有四点：

* 首先，浏览器访问内网的代理服务 IP，代理服务请求前端公网服务，得到一个只有拦截器的 HTML 文件，浏览器解析执行
* 解析到拦截器后，再通过代理服务去公网请求拦截器，拿到拦截器后，在浏览器中注册
* 拦截器注册成功后会自动往 DOM 中添加前端的静态资源
* 这时候后续的所有请求都会经过拦截器拦截和转发，比如前端的静态资源、服务端的 API 请求、第三方服务等

所以，核心职责是拦截器和代理服务的开发，拦截器会拦截所有的网络请求，并改写，首先将这些请求转发到代理服务，并告诉代理服务对应的请求应该转发到哪；代理服务则是解析拦截器转发过来的流量，并将请求转发给对应的公网服务。代理服务本身则是和安全云客户端团队联动，随客户端一块儿就部署下去了，也没有任何额外的成本。

整套方案，只有前端有改动，并且这个改动对线上没有任何影响，其它服务也没有任何感知，而且前端还能和 SaaS 服务完整兼容，技术上也没有任何心智负担。这是一套通用的、轻量的、无成本的 SaaS 服务私有化架构，可以扩展到任意需要私有化访问的 SaaS 服务，自此之后业务也可覆盖到需要私有化部署场景的用户了，增加了业务增长弹性，在这之前业务碰到类似的用户基本上都只能放弃，或者将合适的用户转给公司的其它团队承接，另外在税务局这个项目上也获得了几百万的收益，所以这时候技术对业务来说就是成与不成，如果没有这套方案，这个项目也就只能无奈放弃了。

# 微前端架构

第三件事是微前端架构的实施。既有系统是一套五六年前的技术栈和规范，几年时间，前后超过 20+ 人参与开发，单仓库代码量就超过 40w+ 行，技术债累积严重，但业务却处于高频迭代期，技术侧版本冲突频发，导致业务效率低下。所以，该技术专项背后的诉求就是效率和成本，提升研发效率、系统性能和稳定性，降低系统的维护和上手成本，抛弃历史技术债，多产线并行，提升业务迭代效率。那为什么是微前端架构呢？首先是技术无关性，老系统只做减法，子应用采用全新的技术架构和规范，第二点是增量升级，当时业务需要重构各个功能模块，这样就可以随着业务迭代，以模块为单位拆分子应用，不同的人负责不同的子应用，随着业务的迭代，这样一个庞大的业务系统，在技术侧却进化成了一个个极简的技术项目。

这是一套完成的微前端架构方案，首先是无感知的架构设计，微前端架构的引入，相对于单个应用来说肯定会增加整体的技术复杂度，但通过支持 dev 模式，让测试环境的主应用加载本地开发环境子应用，这样开发时就只需要启动一个应用就可以了，另外就是通过平台化策略，实现子应用的管理和配置，每次新增和调整子应用只需要在平台上配置即可，不需要动主应用，这样就整体去除了微前端架构的复杂性，降低开发和维护成本、提升开发效率。经过不断的迭代，将一个巨石应用拆解成了一个个独立的小型应用，并通过全新的技术栈和 一系列 UI 库全面提效，多业务线并行互不干扰。

另外还有一些增量价值，一是从该专项中沉淀出两套拥有完整规范的项目模版，目前安全云的所有项目的技术栈和规范都是统一的；二是目前公司都在使用的 Webpack5 和 Vite 上传 CDN 的插件，为公司的前端项目提供静态资源上 CDN 的能力。三是性能优化，目标是优化首屏性能，一是常规的优化手段，如按需和懒加载，二是极致的拆包策略，比如同步包、异步包，这两个优化手动是为了尽量去减少首屏的包体积，充分利用网络的并发性，三个网络层面的优化，去除域名发散，利用 HTTP2 的并发能力，避免 TCP 的带宽竞争问题，四是利用微前端架构的预加载能力，优化之后，主应用性能提升 80% 以上，达到秒开，子应用做到首屏直出的效果。

# 稳定性治理体系

第四件事是稳定性治理体系的建设。提到稳定性相信大家都会想到监控、告警、Sentry、自研这几个词，尤其是 Sentry，业界的知名成熟方案，我这边选了 Sentry 和 自研。大家可能有疑问，有了 Sentry 还不够吗？答案是不够，首先是有效报警，系统太老，报警太多，导致有效报警却很少，第二点是告警的及时性，比如低峰时段 + 采样率，就很难命中告警策略，另外一些低流量，但却很重要的模块，也很难命中告警规则，这就导致很多线上问题都依赖用户反馈，有反馈，基本上已经是事故了，第三点是精准检测，比如页面是否正常，页面的某个功能是否正常，某个模块的 UI 呈现是否符合预期等。那怎么实现呢？我们需要一个 7 * 24 小时的自动巡检服务，解决报警不及时的问题，另外需要她像 QA 一样，自动回归主流程，关注核心模块是否有异常，从而解决有效报警和精准检测的问题。

自动巡检，第一个需要解决的难题就是实现 360 账号的自动登录，但 360 账号登录会出滑块验证码，所以需要实现滑块验证的自动验证，有两个关键点，一是识别滑块缺口左直边的位置，计算出滑块的拖动距离，二是绕过系统对滑块行为轨迹的检测，从而实现自动验证的能力。

这是整个稳定性治理体系的架构，分为三部分，主动巡检服务，具备 UI 检测、网络异常检测、核心流程的自动巡检等能力，基本上只要服务报警了，那系统一定是出问题了，需要立马关注并解决，但它有个缺陷是环境单一，但线上环境却五花八分，尤其是我们的用户，都是企业用户，网络环境复杂，经常出现页面不可用的情况，于是增加了线上白屏检测的能力，能适配 SPA 应用、微前端架构、混合架构、带骨架屏应用，并结合运营的客户成功团队，将检测到的有价值的异常用户给到运营，为一线运营提供建联客户的抓手，至于 Sentry，在整个体系中只起辅助作用，用于收集线上问题，帮助我们将系统建设的更稳定。

整套方案下来，线上全年 0 事故，并且多次主动发现服务不可用的情况，包括前端的、服务端的和对接的第三方服务，线上白屏检测也为一线运营同学提供了建联客户的抓手，更好的服务用户，其中 360 自动登录是整个方案的关键和难点。

# 总结

好了，到这里我的个人述职就要结束了，最后再简单总结一下：

* 一是打造高效的 PDF 生成系统，这是一套基于前后端协同设计的方案，业务效率提升至少 50% 以上，并且 100% 还原设计稿，也填补了社区的技术空白，并服务于公司的多条业务线
* 二是设计了 SaaS 私有化一体化架构方案，这是一套通用的、轻量的、无成本的方案，支撑企业安全云 SaaS + 私有化双重能力，满足多类型用户诉求，提升交付效率和业务增长弹性
* 三是微前端架构的落地，这是一套完整的架构方案，并为团队构建了统一的技术栈和开发规范，显著提升技术架构的扩展性和可维护性以及业务迭代速度
* 四是建设系统稳定性治理体系，从技术与流程双维度推动稳定性建设，实现全年 0 事故
* 另外两点因为时间问题，就没在述职报告中讲述
  * 一是直接贡献业务指标，牵头孵化多个关键需求，贡献业务核心 KPI 近 10%，这点在技术团队中应该是非常少见的
  * 二是培养团队两位核心成员成功晋升，一次性通过

# 文化价值观

公司的文化价值观的解读我就不赘述了，主要通过案例来讲实践。

首先是使命必达和开放协作，这在纳米 AI 知识库上体现的淋漓尽致。纳米 AI 知识库：

1. 多部门合作的一个项目，PC 安全与办公、云盘、RAG、浏览器、智能搜索，共5个部门12个团队
2. 前端任务重、复杂度高，因为除了前端本职工作外还需要在业务上串联各方
3. 时间紧、节奏快，一周一版，200+ 需求、70+次发版，十几次通宵，一个月内完成知识库体系的搭建并超越竞品，连轴转和加班到凌晨是常态，

但即使在这样的前提下，为了提升各个场景入口的对接效率，方便各部门快速对接，主动承担了知识库 SDK 开发的相关工作，当前 SDK 在 Chat、RAG、MCP等5大场景中均提供底层能力支持，为每个团队节省了两周的工作量。

另外就是创新突破在 PDF 生成系统和 SaaS 业务私有化架构中有很明显的体现，就不赘述了。

最后的影响力这块儿，一个是 PDF 生成系统，对内赋能多条业务线，对外填补了社区的技术空白，获得了近千次的点赞和收藏。另外就是在社区的技术输出，全网有 1w 的粉丝。

# Q&A 环节

## PDF 生成系统的目录页方案

目录项的页码 = Math.ceil(锚点元素距离页面顶部的高度 / PDF 一页的高度)

比如，PDF 一页的高度是 1123px，锚点 x 距离页面顶部是 1000px，这时候页码就是 1

但锚点一般就像书的大章节一样，每个章节都会新起一页开始，这个在实现的时候其实是通过给这个元素增加一个 break-before: page 的 css 样式来实现的，意思如果在打印的时候遇到一个元素有该样式，就会新开一页面来打印，这个效果就像 Word 文档中的分页符一样，另外这个样式只在打印时生效，页面正常渲染的时候是不起作用的

那这时候再拿锚点 x 来说，刚才计算出来的页码是 1，但实际打印出来却是在第二页，那就需要想办法来实现这个效果了。

首先，假设这个元素刚好是在 1124px 的位置，那计算出来的页码直接就是 2了，那就需要想办法来构造这个结果。

第一个方案是在锚点 x 前面填充一个高度为 123px 的空白元素，这样就可以把锚点 x 给顶到 1124px 的位置了，计算结果自然就对了，但这个方案容错性会比较低，因为一旦计算出现任何 1px 的误差，就会出现一个完全的空白页，比如元素本来就在 1124px 的位置了，但计算错误，导致填充了一个 1px 的空白元素，就会导致锚点 x 在打印时被顶到下一页，中间就出现了空白页，为了解决该方案的容错问题，就有了方案 2，也就是高度补偿算法。

既然锚点 x 距离完美位置差了 123px，那我在计算锚点 x 距离顶部的高度时，就手动补充 124px 的高度，就是 1000px + 124px，最后实际计算页码时就是 Math.ceil(1124 / 1123) => 答案为 2，这就是高度补偿算法的整个推理和设计逻辑。

## PDF 生成系统的沙箱方案

沙箱本质上就是一个隔离环境，避免对系统和其它产生影响。

前端的沙箱分为 JS 沙箱和样式沙箱：

* JS 沙箱一般分为 iframe 和 代理两种方案，代表作有无界和 qiankun
* 而样式沙箱则有 iframe、web component、scoped 三种方案，后两者的代表作有无界、micro-app、qiankun

PDF 生成系统由于其特特殊性 —— 静态的 UI，所以不需要 JS 逻辑，也就是是在合成页可以把动态的 script 都去掉，只保留 HTML + CSS，从而避免逻辑冲突。

而样式沙箱，尝试了两种方案，首先试了 web component，利用 shadow dom 的隔离性，将页面的 HTML、CSS 放到独立的 web component 组件中，从而解决样式污染的问题，目录页的超链接可以通过 JS 从 web component 中取到对应的节点，然后通过 scrollIntoView 滚动到对应的位置，这个方案在浏览器场景中非常完美，可惜的是，页面打印成 PDF 文件之后就丢失了 JS 的能力，也就是丢失了目录页的超链接效果。

另外一个方案就是 scoped，以页面为维度，给页面的所有样式规则增加一个特定的样式选择器来限定其影响范围，和 qiankun 的样式沙箱方案类似。

这个就是 PDF 生成系统的沙箱方案。

## PDF 生成系统高并发场景下的稳定性方案

在 API 和服务之间架了一层 Redis，在 Redis 中设置了一套不同优先级的队列，比如实时队列、定时任务队列、批量任务队列等，实时队列存放 C 端的实时任务，定时任务队列负责存放定时任务，批量任务队列负责存放批量生成任务，优先级策如下：

* 如果定时任务队列为空，优先执行实时队列中的任务
* 如果定时任务队列不为空，则优先执行定时任务队列中的任务，但为了防止实时任务对垒中的任务出现饿死现象，每执行三个定时任务，就会扫一下实时任务队列，有任务就取出一个来执行
* 如果前面两个队列都为空，则执行批量任务队列中的任务

另外，任务如果执行失败，则放回队尾，待重新执行

## SaaS 业务私有化架构，为什么不通过网络层面解决？？

网络层面解决，其实就是刚才将的方案 2，这套方案对我们来说成本很低，但对用户来说成本就很高了，而且由于我们用户的特殊性 —— 中小微企业，技术能力普遍比较薄弱，这个实施和后续的维护成本对双方来说就很高了，而且在项目中可行性也很低，用户根本就不乐意。

而安全云客户端本身就有一种代理模式，应对的也是局域网环境，大体逻辑就是在代理模式下，团队内的内网终端上行流量都会先经过代理服务器，由代理服务器作为统一出口，也方便用户对我们的流量做监管。

## SaaS 业务私有化架构代理服务的负载均衡和安全策略是如何设计的？

准确来说，该方案中没有负载均衡的逻辑，为什么没有呢？

一是受客户端代理模式的架构设计的限制，二是单独实施一套，会增加整个方案的实施复杂度，三是再结合用户使用场景，浏览器通过内网 IP 访问服务，端和服务之间就是一对一的形式，所以没有实施该方案的必要性。

但服务本身其实是有多套，因为代理节点可以有多个，一个代理节点其实就是一套服务。这个能保证用户正常使用。

## SaaS 私有化为何选择拦截器方案而非 Nginx 反向代理

nginx 反向代理没问题，但它代替的是代理服务，而不是拦截器，nginx 做代理的前提是流量得先等到达 nginx，这个就是拦截器的价值。

## 微前端架构拆解过程中遇到的技术挑战及解决方案

解决在 qiankun 微前端架构下，VueRouter@3 和 VueRouter@4 版本混用时产生的兼容性问题。具体来说：

1. qiankun 的沙箱机制 ：qiankun 微前端框架使用的是浅层沙箱，只代理了 window 对象本身，而没有深度代理 window 对象的所有属性和方法。这意味着 window.history 及其状态在主应用和子应用之间是共享的。
2. VueRouter 版本差异 ：
   
   - VueRouter@3（Vue2使用）：每次路由切换时会随机生成一个 history.state = { key: xxx } 的记录
   - VueRouter@4（Vue3使用）：对 history.state 的处理方式不同，它期望 history.state 中有 current 和 back 等属性
3. 问题表现 ：当主应用和子应用使用不同版本的 Vue Router 时，由于 window.history.state 被共享，VueRouter@3 的操作会导致 VueRouter@4 在历史记录中插入一条 undefined 的路由记录，从而导致路由异常。
### 解决方案

```javascript
if (!window.history.state.current) {
    const regex = /^\/(data|welcome|terminal|assets|behavior|trust|dns)/
    window.history.state.current = to.path.replace(regex, '')
    window.history.state.back = from.path.replace(regex, '')
  }
```

这段代码的解决方案是：

1. 检测 window.history.state.current 是否存在，如果不存在（说明可能是 VueRouter@3 修改了 state）
2. 通过正则表达式 /^\/(\/data|welcome|terminal|assets|behavior|trust|dns)/ 匹配并移除路径前缀
3. 手动设置 window.history.state 的 current 和 back 属性，使其符合 VueRouter@4 的期望格式：

   - current ：当前路由路径（去除前缀）
   - back ：上一个路由路径（去除前缀）

这样做可以确保在 qiankun 微前端架构下，即使主应用和子应用使用不同版本的 Vue Router，也能正常工作而不会出现路由异常。

### 技术原理

1. History API ：浏览器的 History API 允许开发者操作浏览器的历史记录，包括 pushState() 和 replaceState() 方法，以及 history.state 属性。
2. Vue Router 的实现 ：Vue Router 基于 History API 实现路由管理，不同版本对 history.state 的处理方式有所不同。
3. qiankun 的沙箱机制 ：qiankun 只对 window 对象本身进行了代理，而没有深度代理 window.history ，导致主应用和子应用共享同一个 history.state 。
这段代码是一个典型的微前端架构中处理不同框架版本兼容性的例子，通过在路由守卫中手动调整 history.state ，确保了不同版本 Vue Router 的正常工作。

另外就是整个架构升级也不是一蹴而就的，技术架构的升级不能影响到业务的迭代，所以整个升级过程也是一个渐进式的升级，loadMicroApp -> registerMicroApps

## 滑块验验证码自动验证

就两个关键点：

* 找到滑块缺口的左直边的位置，从而计算出滑块的拖动距离
* 模拟人力拖动滑块的行为轨迹，从而绕过系统对于自动化工具的检测

怎么找到缺口位置？先分析下这个滑块底图，会发现缺口位置颜色比较深，所以方案就是将滑块底图做黑白二色处理，处理之后底图就只有黑白两种颜色，并且缺口位置一定是大面积的黑色，这时候对这些像素点做纵向的遍历，找到当前列黑色像素最多，上一列白色像素最多的位置，这个位置就是我们要找的左直边的位置，有了位置，接下来就是模拟人拖动滑块的行为轨迹了。

人拖动滑块的行为轨迹有这么几个特点：

* 先慢、后快、再慢
* 拖动时会有抖动
* 另外就是误差

通过二阶贝塞尔曲线来模拟人的拖动行为，但需要注意的是需要在控制点上需要增加随机性，拖动过程中也增加随机的停顿，以模拟人在拖动滑块时的抖动情况和略微停顿

P(t)，t => [0, 1]，步长 0.01，P0 为起点，P1 为控制点，P2 为终点

P(t) = Math.pow(1 - t, 2) * P0.x + 2 * t * (1 - t) * P1.x + Math.pow(t, 2) * P2.x

```javascript
function getBezierPoints(start, end, control) {
  const points = [];
  for (let t = 0; t <= 1; t += 0.01) {
    const x = Math.pow(1 - t, 2) * start.x + 2 * t * (1 - t) * control.x + Math.pow(t, 2) * end.x;
    const y = Math.pow(1 - t, 2) * start.y + 2 * t * (1 - t) * control.y + Math.pow(t, 2) * end.y;
    points.push({ x, y });
  }
  return points;
}
// 示例调用：
const start = { x: 0, y: 50 }; 
const end = { x: 200, y: 50 }; // 假设需滑动 200px
const control = { x: 80, y: 55 }; // 控制点，让轨迹有小波动
const trackPoints = getBezierPoints(start, end, control);
```

这就是滑块自动验证的整个思考逻辑和实施过程。

## 白屏检测 SDK 的实现

* 一是检测时机，当监听到变化后 dispatch 一个自定自定义事件，触发检测逻辑

  * 重写 pushState、replaceState 拦截各个路由框架的路由切换。

  * 监听 popstate，拦截浏览器的前进、后退

  * 监听 hashchange，拦截 hash 路由

    > 需要确保前后 URL 发生了变化
    >
    > 需要通过 e.isTrusted 过滤掉有些框架手动触发的情况

  * load 事件之后触发检测

  * document.readyState === 'complete' 时触发检测，是为了解决代码执行时机发生在 load 事件之后

* 二是检测方式，在目标节点内做十字架检测，比如横竖各取 10个点，如果一半以上的采样点都是目标元素，那目标节点大概率就是白屏了，然后做异常上报。另外为了防止检测异常，检测也做了 3次重试，都是白屏，则上报，这是为了兼容网络异常、解析太慢、性能太差的情况。

